{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPqc2R1Y6L7n",
        "outputId": "e7a73351-6fd5-4d1f-bfdb-45413f69402c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, callbacks, regularizers\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# Set paths to your dataset directories\n",
        "dataset_dir = r'/content/drive/MyDrive/dataset617_all_image/training_set'\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "# Helper functions (unchanged)\n",
        "def handle_remove_readonly(func, path, exc):\n",
        "    import stat\n",
        "    os.chmod(path, stat.S_IWRITE)\n",
        "    func(path)\n",
        "\n",
        "def remove_dir_with_retry(path):\n",
        "    retries = 3\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            shutil.rmtree(path, onerror=handle_remove_readonly)\n",
        "            print(f\"Successfully deleted: {path}\")\n",
        "            break\n",
        "        except PermissionError as e:\n",
        "            print(f\"PermissionError: {e}. Retrying in 2 seconds... ({i+1}/{retries})\")\n",
        "            time.sleep(2)\n",
        "    else:\n",
        "        print(f\"Could not delete {path} after {retries} retries.\")\n",
        "\n",
        "for folder in [train_dir, test_dir]:\n",
        "    if os.path.exists(folder):\n",
        "        remove_dir_with_retry(folder)\n",
        "\n",
        "def split_data(data_dir, train_dir, test_dir, train_ratio=0.8):\n",
        "    classes = [cls for cls in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, cls))]\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_dir = os.path.join(data_dir, cls)\n",
        "        images = os.listdir(cls_dir)\n",
        "\n",
        "        np.random.shuffle(images)\n",
        "        train_count = int(len(images) * train_ratio)\n",
        "\n",
        "        train_images = images[:train_count]\n",
        "        test_images = images[train_count:]\n",
        "\n",
        "        for subset, subset_images in zip([train_dir, test_dir], [train_images, test_images]):\n",
        "            subset_cls_dir = os.path.join(subset, cls)\n",
        "            os.makedirs(subset_cls_dir, exist_ok=True)\n",
        "            for img in subset_images:\n",
        "                shutil.copy(os.path.join(cls_dir, img), os.path.join(subset_cls_dir, img))\n",
        "\n",
        "split_data(dataset_dir, train_dir, test_dir)\n",
        "\n",
        "# Data augmentation and data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load the VGG19 model\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze the pre-trained layers\n",
        "\n",
        "# Define the custom model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(7, activation='softmax')  # Adjust for your number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = callbacks.ModelCheckpoint('best_model_vgg19.keras', save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[model_checkpoint, reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {test_accuracy:.2f}')\n",
        "\n",
        "# Analyze training history\n",
        "history_df = pd.DataFrame({\n",
        "    \"Epoch\": range(1, len(history.history['accuracy']) + 1),\n",
        "    \"Train Accuracy\": history.history['accuracy'],\n",
        "    \"Validation Accuracy\": history.history['val_accuracy'],\n",
        "    \"Train Loss\": history.history['loss'],\n",
        "    \"Validation Loss\": history.history['val_loss']\n",
        "})\n",
        "print(\"\\nTrain and Validation Accuracy and Loss by Epoch:\")\n",
        "print(history_df)\n",
        "\n",
        "# Generate classification report and confusion matrix\n",
        "test_generator.reset()\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = test_generator.classes\n",
        "\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "classification_report_dict = classification_report(true_labels, predicted_labels, target_names=class_labels, output_dict=True)\n",
        "classification_df = pd.DataFrame(classification_report_dict).transpose()\n",
        "print(\"\\nClass-wise Accuracy and Loss:\")\n",
        "print(classification_df)\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d', xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot metrics\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.axhline(y=test_accuracy, color='r', linestyle='--', label='Final Test Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.axhline(y=test_loss, color='r', linestyle='--', label='Final Test Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L_F-iLLv6QkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}